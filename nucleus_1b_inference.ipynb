{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zNtVtI3RSH4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f640c8-789b-40c9-f3c2-2c172d147759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/270.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/270.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g6c2XDlfSgAC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAr9h2aIXtK5"
      },
      "source": [
        "## Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zGF7bfgdSm9w"
      },
      "outputs": [],
      "source": [
        "model_name_or_id = \"NucleusOrg/Nucleus-1B-alpha-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOk3ZT8fSzxE"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_id, torch_dtype=torch.float16, device_map=\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1cejoSyXxra"
      },
      "source": [
        "## Model Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLoXQ6uFS2sd",
        "outputId": "db3cf79c-9599-4208-be12-1f1e9445a217"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x MistralDecoderLayer(\n",
              "        (self_attn): MistralAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): MistralRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm()\n",
              "        (post_attention_layernorm): MistralRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXwNn5ynyBKr"
      },
      "source": [
        "## Prompt Format\n",
        "\n",
        "There is no certain format. But _textbook_ style is encouraged.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "### Lesson: Python Programming 101\\n### Introduction\\n\n",
        "```\n",
        "\n",
        "So in the below cell, you can easily modify the prompt without messing with the format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eDKNRtBvTtlO"
      },
      "outputs": [],
      "source": [
        "prompt = \"### Lesson: Python Programming 101\\n### Introduction\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D2LZicvNTwwP"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni3F1Sm1yYz_"
      },
      "source": [
        "### Generation Config\n",
        "\n",
        "This cell, is a simple and easy way to tweak the configurations for text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8NK_6nTcTzn-"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    top_k=1,\n",
        "    temperature=0.9,\n",
        "    max_new_tokens=500,\n",
        "    repetition_penalty=1.5,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PuDza4_T1iV",
        "outputId": "2d7ddd82-61f9-45b3-fd2b-d8e71a8e22fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Lesson: Python Programming 101\n",
            "### Introduction\n",
            "In this lesson, we will learn how to program a simple program in Python. We will cover the basics of programming languages and their syntax for creating software programs. By the end of this lesson, you should be able to write your own code using different libraries or frameworks.\n",
            "### Section 1: Basics of Programming Languages\n",
            "A function is an object that performs calculations on variables. It takes two arguments - one argument (the \"Hello\" followed by another) as it executes the same operation. Here are some basic concepts related to programming languages:\n",
            "- Variables: A variable is named storage locations where data can hold values such as numbers, text, and other information. For example, `a` would declare a value called \"my_numbers\".\n",
            "- Control Structures: Contingency lists allow us to repeat operations with new inputs without any intervention from the computer. They provide additional features like functions and methods.\n",
            "- Functions: An event is a sequence of instructions that tells the browser what happens next when certain conditions occur. In our daily lives, we use various tools and techniques to perform tasks efficiently.\n",
            "### Section 2: Practical Examples\n",
            "Let's take a look at some practical examples of how to create a program in Python:\n",
            "```python\n",
            "# Create a program in Java\n",
            "def get(x):\n",
            "    # Print out the result\n",
            "print(\"The name of the program:\", x)\n",
            "```\n",
            "This program uses the following output:\n",
            "```\n",
            "500\n",
            "6439\n",
            "7844\n",
            "```\n",
            "Now let's try writing a program in Python:\n",
            "```python\n",
            "import math\n",
            "radius = float(input(\"Enter the number of rows representing the unknown value?))\n",
            "area = input(\"pi\", area)\n",
            "print(\"The total cost of the program has been steadily increasing over time.\n",
            "```\n",
            "We want to print out the sum of all possible outcomes between each row.\n",
            "Here's an example of how we could calculate the total cost per square:\n",
            "```python\n",
            "from b import b\n",
            "for dictionaries in b\n",
            "while True:\n",
            "    # Calculate the total cost per square\n",
            "    draw up the equation:\n",
            "        if r == y\":\n",
            "            return r + c while dictionaries are given the first digit of the original work.\n",
            "    \n",
            "    # Add more details to make\n"
          ]
        }
      ],
      "source": [
        "outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}